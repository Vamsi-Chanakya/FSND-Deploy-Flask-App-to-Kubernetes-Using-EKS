1) Step 1 - Login to AWS CLI

    $ aws configure --profile default

    $ aws configure list
              Name                    Value             Type    Location
              ----                    -----             ----    --------
           profile                <not set>             None    None
        access_key     ****************3GK2 shared-credentials-file
        secret_key     ****************m7Fk shared-credentials-file
            region                us-west-2      config-file    ~/.aws/config


2) Create an EKS cluster

    $ eksctl create cluster --name simple-jwt-api

    [ℹ]  eksctl version 0.19.0
    [ℹ]  using region us-west-2
    [ℹ]  setting availability zones to [us-west-2a us-west-2b us-west-2c]
    [ℹ]  subnets for us-west-2a - public:192.168.0.0/19 private:192.168.96.0/19
    [ℹ]  subnets for us-west-2b - public:192.168.32.0/19 private:192.168.128.0/19
    [ℹ]  subnets for us-west-2c - public:192.168.64.0/19 private:192.168.160.0/19
    [ℹ]  nodegroup "ng-8f948733" will use "ami-025bf02d663404bbc" [AmazonLinux2/1.15]
    [ℹ]  using Kubernetes version 1.15
    [ℹ]  creating EKS cluster "simple-jwt-api" in "us-west-2" region with un-managed nodes
    [ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup
    [ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-west-2 --cluster=simple-jwt-api'
    [ℹ]  CloudWatch logging will not be enabled for cluster "simple-jwt-api" in "us-west-2"
    [ℹ]  you can enable it with 'eksctl utils update-cluster-logging --region=us-west-2 --cluster=simple-jwt-api'
    [ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "simple-jwt-api" in "us-west-2"
    [ℹ]  2 sequential tasks: { create cluster control plane "simple-jwt-api", create nodegroup "ng-8f948733" }
    [ℹ]  building cluster stack "eksctl-simple-jwt-api-cluster"
    [ℹ]  deploying stack "eksctl-simple-jwt-api-cluster"
    [ℹ]  building nodegroup stack "eksctl-simple-jwt-api-nodegroup-ng-8f948733"
    [ℹ]  --nodes-min=2 was set automatically for nodegroup ng-8f948733
    [ℹ]  --nodes-max=2 was set automatically for nodegroup ng-8f948733
    [ℹ]  deploying stack "eksctl-simple-jwt-api-nodegroup-ng-8f948733"
    [ℹ]  waiting for the control plane availability...
    [✔]  saved kubeconfig as "/Users/vamsi/.kube/config"
    [ℹ]  no tasks
    [✔]  all EKS cluster resources for "simple-jwt-api" have been created
    [ℹ]  adding identity "arn:aws:iam::417071062939:role/eksctl-simple-jwt-api-nodegroup-n-NodeInstanceRole-1XQJO43FLSE0T" to auth ConfigMap
    [ℹ]  nodegroup "ng-8f948733" has 0 node(s)
    [ℹ]  waiting for at least 2 node(s) to become ready in "ng-8f948733"
    [ℹ]  nodegroup "ng-8f948733" has 2 node(s)
    [ℹ]  node "ip-192-168-8-91.us-west-2.compute.internal" is ready
    [ℹ]  node "ip-192-168-90-225.us-west-2.compute.internal" is ready
    [ℹ]  kubectl command should work with "/Users/vamsi/.kube/config", try 'kubectl get nodes'
    [✔]  EKS cluster "simple-jwt-api" in "us-west-2" region is ready

3) Set a secret using AWS parameter store

    $ aws ssm delete-parameter --name JWT_SECRET

    $ aws ssm put-parameter --name JWT_SECRET --value "YourJWTSecret" --type  SecureString

    {
        "Version": 1,
        "Tier": "Standard"
    }

4) Create an additional role and fetch AWS auth file

    $ ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)

    $ echo $ACCOUNT_ID
        417071062939

    $ TRUST="{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\",  \"Principal\": { \"AWS\": \"arn:aws:iam::${ACCOUNT_ID}:root\" }, \"Action\":  \"sts:AssumeRole\" } ] }"

    $ echo $TRUST
        { "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Principal": { "AWS": "arn:aws:iam::417071062939:root" }, "Action": "sts:AssumeRole" } ] }

    $ aws iam create-role --role-name UdacityFlaskDeployCBKubectlRole --assume-role-policy-document "$TRUST" --output text --query 'Role.Arn'
        arn:aws:iam::417071062939:role/UdacityFlaskDeployCBKubectlRole

    $ echo '{ "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Action": [ "eks:Describe*", "ssm:GetParameters" ], "Resource": "*" } ] }' > ./iam-role-policy

    $ cat iam-role-policy
        { "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Action": [ "eks:Describe*", "ssm:GetParameters" ], "Resource": "*" } ] }

    $ aws iam put-role-policy --role-name UdacityFlaskDeployCBKubectlRole --policy-name eks-describe --policy-document file://./iam-role-policy

    $ kubectl get -n kube-system configmap/aws-auth -o yaml > ./aws-auth-patch.yml

5) Configure the aws-auth-patch.yml file with the new role

    $ cat aws-auth-patch.yml
        apiVersion: v1
        data:
          mapRoles: |
            - groups:
              - system:bootstrappers
              - system:nodes
              rolearn: arn:aws:iam::417071062939:role/eksctl-simple-jwt-api-nodegroup-n-NodeInstanceRole-1XQJO43FLSE0T
              username: system:node:{{EC2PrivateDNSName}}
            - groups:
              - system:masters
              rolearn: arn:aws:iam::417071062939:role/UdacityFlaskDeployCBKubectlRole
              username: build
          mapUsers: |
            []
        kind: ConfigMap
        metadata:
          creationTimestamp: "2020-06-23T00:02:45Z"
          name: aws-auth
          namespace: kube-system
          resourceVersion: "795"
          selfLink: /api/v1/namespaces/kube-system/configmaps/aws-auth
          uid: 28098fd4-dd19-4378-a1d4-0e82f8c08dcb

6) Patch the modified aws-auth-patch.yml

    $ kubectl patch configmap/aws-auth -n kube-system --patch "$(cat ./aws-auth-patch.yml)"
        configmap/aws-auth patched

7) Fill the ci-cd-codepipeline.cfn.yml file

    EksClusterName  : simple-jwt-api
    GitSourceRepo   : FSND-Deploy-Flask-App-to-Kubernetes-Using-EKS
    GitBranch       : master
    GitHubUser      : Vamsi-Chanakya
    KubectlRoleName : UdacityFlaskDeployCBKubectlRole

8) Create the CloudFormation stack

    Stack name      : simple-jwt-api-stack-test
    Access token    : d08479206c1cfeb4bc94c855121ed77a04425184
    Repository      : FSND-Deploy-Flask-App-to-Kubernetes-Using-EKS
    Branch          : master
    Docker image    : aws/codebuild/docker:17.09.0
    kubectl IAM role: UdacityFlaskDeployCBKubectlRole
    EKS cluster name: simple-jwt-api

9) Create a pipeline watching for commits to your Github repository

10) Build and deploy your image using CodeBuild

11) Grab the EKS Cluster endpoint URL

    $ kubectl get services simple-jwt-api -o wide
    NAME             TYPE           CLUSTER-IP       EXTERNAL-IP                                                               PORT(S)        AGE     SELECTOR
    simple-jwt-api   LoadBalancer   10.100.251.142   a640d07e4190841249c24acba03e9bab-1473204004.us-west-2.elb.amazonaws.com   80:31511/TCP   6m51s   app=simple-jwt-api

12) Test endpoints

    $ export URL="a640d07e4190841249c24acba03e9bab-1473204004.us-west-2.elb.amazonaws.com"

    $ export TOKEN=`curl -d '{"email":"vamsi.chanakya@gmail.com","password":"Enter pwd at run time"}' -H "Content-Type: application/json" -X POST $URL/auth  | jq -r '.token'`
          % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                         Dload  Upload   Total   Spent    Left  Speed
        100   246  100   188  100    58   1074    331 --:--:-- --:--:-- --:--:--  1080

    $ curl --request GET $URL/contents -H "Authorization: Bearer ${TOKEN}" | jq

          % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                         Dload  Upload   Total   Spent    Left  Speed
        100    71  100    71    0     0    701      0 --:--:-- --:--:-- --:--:--   702
        {
          "email": "vamsi.chanakya@gmail.com",
          "exp": 1594083584,
          "nbf": 1592873984
        }

   Important Reference : https://github.com/jungleBadger/FSND-Deploy-Flask-App-to-Kubernetes-Using-EKS/blob/master/troubleshooting/deploy.md#step-4---create-an-additional-role-and-fetch-aws-auth-file

13) Add running tests as part of the build

        pre_build:
            commands:
                - pip3 install -r requirements.txt
                - pytest test_main.py


14) You can check the tests prevent a bad deployment by breaking the tests on purpose